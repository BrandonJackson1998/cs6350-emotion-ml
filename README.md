# cs6350-emotion-ml
Detecting change in emotion given a time series of images

## ðŸš€ Quick Start

### Running Your First Experiment
```bash
# Basic balanced experiment
python3 scripts/run_experiment.py --name "my_first_experiment" --epochs 20

# Experiment with sadness focus
python3 scripts/run_experiment.py --name "sadness_focus" --epochs 20 --sad 5.0 --happy 0.5

# Fear and sadness focus
python3 scripts/run_experiment.py --name "fear_sad_focus" --epochs 25 --fear 3.0 --sad 3.0 --happy 0.3
```

### Continuing Training with Ratio Manipulation
```bash
# Resume from checkpoint with same settings
python3 scripts/run_experiment.py \
    --resume "./experiments/my_experiment_20250108_143022/checkpoint_epoch_5.pt" \
    --name "resumed_experiment" \
    --epochs 10

# Resume with different emotion focus (ratio manipulation)
python3 scripts/run_experiment.py \
    --resume "./experiments/my_experiment_20250108_143022/checkpoint_epoch_5.pt" \
    --name "happiness_focus" \
    --epochs 15 \
    --happy 4.0 --sad 0.3 --surprise 2.0

# Resume with completely different distribution
python3 scripts/run_experiment.py \
    --resume "./experiments/sadness_focus_20250108_143022/checkpoint_epoch_10.pt" \
    --name "balanced_resume" \
    --epochs 10 \
    --description "Resumed with balanced training after sadness focus"
```

### Project Structure
```
cs6350-emotion-ml/
â”œâ”€â”€ src/                    # Core ML code
â”‚   â””â”€â”€ benchmark.py        # Enhanced benchmark with checkpointing
â”œâ”€â”€ scripts/               # Training and experiment scripts
â”‚   â””â”€â”€ run_experiment.py   # ðŸŽ¯ Universal experiment runner
â”œâ”€â”€ experiments/           # Experiment outputs (auto-created)
â”‚   â””â”€â”€ *.pt files         # PyTorch checkpoints (git-ignored, ~7GB saved!)
â”œâ”€â”€ data/                 # Dataset (train/test splits)
â””â”€â”€ requirements.txt      # Dependencies
```

**ðŸ’¾ Space Optimization**: PyTorch model files (`.pt`) are git-ignored to save ~7GB of repository space. These files can be regenerated by running experiments.

## ðŸ› ï¸ Generic Experiment Runner

The new `scripts/run_experiment.py` provides a unified interface for all emotion recognition experiments:

### Key Features
- **ðŸŽ¯ Flexible Emotion Weighting**: Control sampling weights for each emotion
- **ðŸ”„ Checkpoint Resumption**: Resume from any checkpoint with new settings
- **ðŸ“Š Complete Experiment Tracking**: Automatic logging and result saving
- **âš¡ Optimized Training**: Based on plateau analysis (10-epoch recommendations)

### Command Line Options
```bash
# Basic options
--name "experiment_name"     # Experiment name (for new experiments)
--epochs 20                  # Number of training epochs
--resume "path/to/checkpoint" # Resume from checkpoint
--list                       # List available checkpoints

# Emotion sampling weights (all default to 1.0)
--angry 1.0 --disgust 1.0 --fear 1.0 --happy 1.0
--neutral 1.0 --sad 1.0 --surprise 1.0

# Model configuration
--model "dima806/facial_emotions_image_detection"
--batch-size 32 --learning-rate 2e-5 --samples-per-class 100

# Additional options
--description "Custom description"
--device "auto"  # auto, cpu, cuda
```

### Example Workflows

**1. New Experiment with Custom Focus:**
```bash
python3 scripts/run_experiment.py \
    --name "sadness_focus" \
    --epochs 10 \
    --sad 5.0 --fear 2.0 --happy 0.5 \
    --description "Focus on sadness and fear emotions"
```

**2. Resume with Different Focus:**
```bash
python3 scripts/run_experiment.py \
    --resume "./experiments/sadness_focus_20250108_143022/checkpoint_epoch_5.pt" \
    --name "happiness_focus" \
    --epochs 10 \
    --happy 4.0 --surprise 2.0 --sad 0.3 \
    --description "Switched focus to happiness after sadness training"
```

**3. Balanced Training:**
```bash
python3 scripts/run_experiment.py \
    --name "balanced_training" \
    --epochs 15
    # All weights default to 1.0 for balanced training
```

**4. List Available Checkpoints:**
```bash
python3 scripts/run_experiment.py --list
```

## ðŸ“Š First Run - Benchmark Results

### Model Performance
- **Overall Accuracy**: 61.14%
- **Training Setup**: 100 samples per class (700 total)
- **Validation Set**: 100 samples per class (700 total)
- **Epochs**: 5
- **Model**: dima806/facial_emotions_image_detection

### Classification Report

| Emotion  | Precision | Recall | F1-Score | Support |
|----------|-----------|--------|----------|---------|
| Angry    | 0.4390    | 0.5400 | 0.4843   | 100     |
| Disgust  | 0.8972    | 0.9600 | 0.9275   | 100     |
| Fear     | 0.3934    | 0.2400 | 0.2981   | 100     |
| Happy    | 0.8370    | 0.7700 | 0.8021   | 100     |
| Neutral  | 0.5088    | 0.5800 | 0.5421   | 100     |
| Sad      | 0.3981    | 0.4100 | 0.4039   | 100     |
| Surprise | 0.7800    | 0.7800 | 0.7800   | 100     |
| **Macro Avg** | **0.6076** | **0.6114** | **0.6054** | **700** |

### Key Insights

**Strong Performance:**
- ðŸŽ­ **Disgust**: 96% recall, 90% precision (best performing)
- ðŸ˜Š **Happy**: 77% recall, 84% precision
- ðŸ˜² **Surprise**: 78% recall and precision

**Weak Performance:**
- ðŸ˜¨ **Fear**: 24% recall, 39% precision (worst performing)
- ðŸ˜¢ **Sad**: 41% recall, 40% precision
- ðŸ˜  **Angry**: 54% recall, 44% precision

**Observations:**
- Model excels at distinct expressions (disgust, surprise, happy)
- Struggles with subtle or ambiguous emotions (fear, sad, angry)
- Overall performance (61.14%) significantly exceeds random baseline (14.3%)

## ðŸ”„ Enhanced Checkpoint System

This project includes an advanced checkpoint system that allows you to:

### âœ… **Complete Training State Preservation**
- Save model weights, optimizer state, epoch number, best accuracy, and training history
- Resume training from any saved checkpoint
- Never lose progress due to interruptions

### ðŸ“Š **Training Ratio Manipulation**
- Change sampling weights between experiments
- Track how different data ratios affect model performance
- Maintain experiment lineage and change logs

### ðŸŽ¯ **Key Features**
- **Experiment Isolation**: Each experiment gets its own timestamped folder
- **Ratio Tracking**: Logs when sampling weights change between experiments
- **Resume Capability**: Continue from any checkpoint with same or different configuration
- **Backward Compatibility**: Works with existing model files

### ðŸ“ **Experiment Structure**
Each experiment creates:
```
experiments/experiment_name_20250108_143022/
â”œâ”€â”€ experiment_config.json          # Configuration
â”œâ”€â”€ experiment_changes.log           # Ratio change tracking
â”œâ”€â”€ checkpoint_epoch_1.pt           # Complete state checkpoints
â”œâ”€â”€ best_checkpoint.pt              # Best performing model
â”œâ”€â”€ classification_report.txt       # Results
â”œâ”€â”€ confusion_matrix_benchmark.png  # Visualizations
â””â”€â”€ training_history.csv            # Training data
```

### ðŸ› ï¸ **Usage Examples**

**Using the Generic Script (Recommended):**
```bash
# New experiment with custom weights
python3 scripts/run_experiment.py \
    --name "my_experiment" \
    --epochs 10 \
    --disgust 3.0 --fear 2.0 --happy 0.8 \
    --description "Testing different sampling weights"

# Resume with different ratios
python3 scripts/run_experiment.py \
    --resume "./experiments/my_experiment_20250108_143022/checkpoint_epoch_5.pt" \
    --name "different_weights_experiment" \
    --epochs 5 \
    --disgust 5.0 --fear 3.0 --happy 0.5 \
    --description "Resumed with different sampling weights"
```

**Using the Python API:**
```python
from src.benchmark import create_experiment_config, run_experiment

config = create_experiment_config(
    experiment_name="my_experiment",
    sampling_weights={
        'disgust': 3.0,  # Higher weight for underrepresented class
        'fear': 2.0,     # Higher weight for underrepresented class
        'happy': 0.8,    # Lower weight for overrepresented class
    },
    num_epochs=10,
    experiment_description="Testing different sampling weights"
)

output_dir, best_acc = run_experiment(config)
```

For detailed documentation, see [docs/CHECKPOINT_SYSTEM.md](docs/CHECKPOINT_SYSTEM.md).

## Project TODO List

### 1. Problem Definition & Goals
- [ ] Define emotion transition detection for educational monitoring
  - Detect transitions between 7 emotional states: Angry, Disgust, Fear, Happy, Sad, Surprise, Neutral
  - Focus on monitoring students' emotional states during learning sessions
- [ ] Establish success criteria and evaluation metrics
  - Primary metric: Accuracy of emotion transition detection
  - Secondary metric: Processing speed for real-time application
- [ ] Define target emotions: Angry, Disgust, Fear, Happy, Sad, Surprise, Neutral
- [ ] Set performance benchmarks
  - Target accuracy: >85% for emotion classification
  - Target speed: Real-time processing capability (<100ms per frame)
  - Application: Educational technology for student engagement monitoring

### 2. Data Collection
- [ ] Download FER-2013 dataset from Kaggle
  - Source: https://www.kaggle.com/datasets/msambare/fer2013/data?select=test
  - Target: ~30,000 images for training, ~10,000 for testing
- [ ] Verify dataset contains all required emotion classes (Angry, Disgust, Fear, Happy, Sad, Surprise, Neutral)
- [ ] Document dataset characteristics and limitations
- [ ] Set up data storage structure and organization
- [ ] Validate data integrity and completeness

### 3. Data Preparation
- [ ] Clean and validate image data
- [ ] Preprocess images (resize, normalize, etc.)
- [ ] Handle missing or corrupted data
- [ ] Create train/validation/test splits
- [ ] Format data for model consumption

### 4. Feature Engineering
- [ ] Extract facial features from images
- [ ] Create temporal features from time series
- [ ] Select most informative features
- [ ] Handle feature scaling and normalization
- [ ] Create feature pipelines

### 5. Model Training
- [ ] Select appropriate ML algorithms
- [ ] Implement baseline models
- [ ] Train deep learning models (CNNs, RNNs, etc.)
- [ ] Handle time series aspects of the data
- [ ] Cross-validation setup

### 6. Evaluation
- [ ] Implement evaluation metrics
- [ ] Create test harness
- [ ] Perform model validation
- [ ] Compare model performances
- [ ] Generate evaluation reports

### 7. Hyperparameter Tuning
- [ ] Define hyperparameter search space
- [ ] Implement tuning strategy (grid search, random search, etc.)
- [ ] Optimize model parameters
- [ ] Document best parameters found

### 8. Deployment
- [ ] Package model for deployment
- [ ] Create inference pipeline
- [ ] Build API or application interface
- [ ] Test deployment setup
- [ ] Create documentation for usage

### 9. Monitoring & Maintenance
- [ ] Implement performance monitoring
- [ ] Set up model drift detection
- [ ] Create update/retraining pipeline
- [ ] Plan for model versioning
- [ ] Document maintenance procedures
